{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2fc596",
   "metadata": {},
   "source": [
    "# Demonstration of List appplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064102e",
   "metadata": {},
   "source": [
    "## 1. Standardize Input's Length Using Padding Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eedb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences with zeros so \n",
    "    that all sequences have the same length.\n",
    "\n",
    "    Args:\n",
    "        sequences (List[List[int]]): A list of lists (sequences) of integers.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: A new list where each sequence is padded\n",
    "                with zeros to match the maximum length.\n",
    "    \"\"\"\n",
    "    if not sequences:\n",
    "        return []\n",
    "\n",
    "    max_len = max(len(seq) for seq in sequences)  # Find maximum sequence length\n",
    "\n",
    "    # Pad each sequence without modifying the original\n",
    "    return [seq + [0] * (max_len - len(seq)) for seq in sequences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c75f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Sequences of different lengths\n",
    "assert pad_sequences([[1, 2], [3, 4, 5], [6]])\\\n",
    "    == [[1, 2, 0], [3, 4, 5], [6, 0, 0]], \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: All sequences already have the same length, no padding needed\n",
    "assert pad_sequences([[1, 1], [2, 2], [3, 3]])\\\n",
    "    == [[1, 1], [2, 2], [3, 3]], \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: One empty sequence and one long sequence, the empty sequence is padded\n",
    "assert pad_sequences([[1, 2, 3], []])\\\n",
    "    == [[1, 2, 3], [0, 0, 0]], \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Only one sequence, keep it as is\n",
    "assert pad_sequences([[7, 8]]) == [[7, 8]], \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Input list is empty, return an empty list\n",
    "assert pad_sequences([]) == [], \"Test 5 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa978f",
   "metadata": {},
   "source": [
    "## 2. Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac04ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import random\n",
    "\n",
    "def split_dataset(data: List[Any], \n",
    "                  train_ratio: float, \n",
    "                  val_ratio: float, \n",
    "                  shuffle: bool = False\n",
    "                  ) -> Tuple[List[Any], List[Any], List[Any]]:\n",
    "    \"\"\"\n",
    "    Split a dataset into train, validation, and test sets \n",
    "    according to given ratios.\n",
    "\n",
    "    Args:\n",
    "        data (List[Any]): The dataset to split.\n",
    "        train_ratio (float): Proportion of data \n",
    "                    for the training set (0 <= train_ratio <= 1).\n",
    "        val_ratio (float): Proportion of data \n",
    "                    for the validation set (0 <= val_ratio <= 1).\n",
    "        shuffle (bool): Whether to shuffle the dataset before splitting. \n",
    "                    Default is False.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Any], List[Any], List[Any]]: Train, validation, \n",
    "            and test datasets.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return [], [], []\n",
    "\n",
    "    if not (0 <= train_ratio <= 1 and 0 <= val_ratio <= 1):\n",
    "        raise ValueError(\"train_ratio and val_ratio must be between 0 and 1\")\n",
    "\n",
    "    if shuffle:\n",
    "        data = data.copy()  # avoid modifying original list\n",
    "        random.shuffle(data)\n",
    "\n",
    "    data_len = len(data)\n",
    "    train_end = round(data_len * train_ratio)\n",
    "    val_end = train_end + round(data_len * val_ratio)\n",
    "\n",
    "    train_set = data[:train_end]\n",
    "    val_set = data[train_end:val_end]\n",
    "    test_set = data[val_end:]\n",
    "\n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Standard split ratios\n",
    "dataset = list(range(100))\n",
    "train, val, test = split_dataset(dataset, 0.7, 0.15)\n",
    "assert (len(train), len(val), len(test)) == (70, 15, 15), \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: Small dataset, evenly split, check rounding\n",
    "dataset = ['a', 'b', 'c']\n",
    "train, val, test = split_dataset(dataset, 0.5, 0.5)\n",
    "assert (len(train), len(val), len(test)) == (2, 1, 0), \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: No validation set\n",
    "dataset = list(range(5))\n",
    "train, val, test = split_dataset(dataset, 0.8, 0.0)\n",
    "assert (train, val, test) == (list(range(4)), [], [4]), \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Empty dataset\n",
    "dataset = []\n",
    "train, val, test = split_dataset(dataset, 0.6, 0.2)\n",
    "assert (train, val, test) == ([], [], []), \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Total ratio less than 1, remainder goes to test set\n",
    "dataset = list(range(10))\n",
    "train, val, test = split_dataset(dataset, 0.2, 0.3)  # 2, 3, 5\n",
    "assert (train, val, test) == \\\n",
    "(list(range(2)), list(range(2, 5)), list(range(5, 10))), \"Test 5 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73db887",
   "metadata": {},
   "source": [
    "## 3. Flattening Token for Vocabulary Bag (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19468f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tokens(corpus):\n",
    "    \"\"\"\n",
    "    Flatten a nested list of tokens (list of sentences) \n",
    "    into a single list of tokens.\n",
    "\n",
    "    Args:\n",
    "        corpus (List[List[Any]]): A list of sentences, \n",
    "        each sentence is a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A flat list containing all tokens from all sentences.\n",
    "                   Returns an empty list if the input corpus is empty.\n",
    "    \"\"\"\n",
    "    if not corpus:\n",
    "        return []\n",
    "    return [word for sentence in corpus for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Two sentences, each with multiple words, flatten the entire corpus\n",
    "assert flatten_tokens([[\"hello\", \"world\"], [\"this\", \"is\", \"a\", \"test\"]]) == \\\n",
    "[\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"], \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: One short sentence and one single-word sentence, \n",
    "# check handling of uneven lists\n",
    "assert flatten_tokens([[\"a\", \"b\"], [\"c\"]]) == [\"a\", \"b\", \"c\"], \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: Empty text corpus, output should be an empty list\n",
    "assert flatten_tokens([]) == [], \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Only one sentence, check single sentence behavior\n",
    "assert flatten_tokens([[\"single\", \"sentence\"]]) \\\n",
    "    == [\"single\", \"sentence\"], \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Multiple sentences of different lengths, check stability of result\n",
    "assert flatten_tokens([[\"deep\", \"learning\"], [\"rocks\"], [\"NLP\", \"is\", \"fun\"]]) == \\\n",
    "[\"deep\", \"learning\", \"rocks\", \"NLP\", \"is\", \"fun\"], \"Test 5 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42af53e",
   "metadata": {},
   "source": [
    "## 4. One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "964121b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def one_hot_encode(labels: List[Any], classes: List[Any]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    One-hot encode a list of labels given a list of classes.\n",
    "\n",
    "    Args:\n",
    "        labels (List[Any]): List of labels to encode.\n",
    "        classes (List[Any]): List of all possible classes in order.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: One-hot encoded vectors corresponding to labels.\n",
    "                         Returns an empty list if labels or classes are empty.\n",
    "    \"\"\"\n",
    "    if not labels or not classes:\n",
    "        return []\n",
    "\n",
    "    # Build a dictionary for fast class index lookup\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "    vec_len = len(classes)\n",
    "    result = []\n",
    "\n",
    "    for lb in labels:\n",
    "        if lb not in class_to_idx:\n",
    "            raise ValueError(f\"Label '{lb}' not found in classes\")\n",
    "        vec = [0] * vec_len\n",
    "        vec[class_to_idx[lb]] = 1\n",
    "        result.append(vec)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Full list of labels, check order and correct mapping\n",
    "assert one_hot_encode([\"dog\", \"cat\", \"bird\", \"dog\"], [\"cat\", \"dog\", \"bird\"])\\\n",
    "    == [[0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0]], \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: Empty label list, result should also be empty\n",
    "assert one_hot_encode([], [\"cat\", \"dog\", \"bird\"]) \\\n",
    "    == [], \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: Only two classes, repeated labels multiple times\n",
    "assert one_hot_encode([\"A\", \"A\", \"B\"], [\"A\", \"B\"]) \\\n",
    "    == [[1, 0], [1, 0], [0, 1]], \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Only one label, check single output\n",
    "assert one_hot_encode([\"cat\"], [\"cat\", \"dog\", \"bird\"]) \\\n",
    "    == [[1, 0, 0]], \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Classes with long labels, check that mapping is not \n",
    "# affected by string length\n",
    "assert one_hot_encode([\"sad\", \"happy\"], [\"happy\", \"sad\", \"neutral\"]) \\\n",
    "    == [[0, 1, 0], [1, 0, 0]], \"Test 5 Failed\"\n",
    "    \n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c6da5",
   "metadata": {},
   "source": [
    "## 5. Filter Out Bounding Box Using Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaee8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_confidence_boxes(predictions, threshold):\n",
    "    if not predictions:\n",
    "        return []\n",
    "    if not (0 <= threshold <= 1):\n",
    "        raise ValueError(\"Threshold must be between 0 and 1\")\n",
    "    \n",
    "    return [bbox for bbox in predictions if bbox[1] >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0942af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Two boxes have enough confidence (>= 0.8), one box is filtered out\n",
    "predictions1 = [[0, 0.95, 10, 10, 50, 50], \n",
    "                [1, 0.4, 20, 20, 30, 30], \n",
    "                [0, 0.88, 15, 15, 40, 40]]\n",
    "assert filter_low_confidence_boxes(predictions1, 0.8) == \\\n",
    "[[0, 0.95, 10, 10, 50, 50], [0, 0.88, 15, 15, 40, 40]], \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: Threshold higher than all boxes -> result is empty\n",
    "assert filter_low_confidence_boxes(predictions1, 0.99) == [], \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: Empty input data -> output should also be empty\n",
    "assert filter_low_confidence_boxes([], 0.5) == [], \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: One box with confidence exactly equal to threshold -> still kept\n",
    "predictions2 = [[0, 0.5, 5, 5, 10, 10]]\n",
    "assert filter_low_confidence_boxes(predictions2, 0.5) \\\n",
    "    == [[0, 0.5, 5, 5, 10, 10]], \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: All boxes have enough confidence -> no boxes are filtered out\n",
    "predictions3 = [[0, 0.85, 1, 2, 3, 4], [1, 0.95, 4, 5, 6, 7]]\n",
    "assert filter_low_confidence_boxes(predictions3, 0.5) \\\n",
    "    == predictions3, \"Test 5 Failed\"\n",
    "    \n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dce08",
   "metadata": {},
   "source": [
    "## 6. Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2768f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(data):\n",
    "    \"\"\"\n",
    "    Scale a list of numerical values to the range [0, 1] \n",
    "    using min-max normalization.\n",
    "\n",
    "    Args:\n",
    "        data (List[float]): List of numerical values.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Normalized list of values in [0, 1]. \n",
    "        Returns empty list if input is empty.\n",
    "            If all values are identical, returns a list of zeros.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "\n",
    "    if max_val == min_val:\n",
    "        return [0 for _ in data]\n",
    "\n",
    "    return [(x - min_val) / (max_val - min_val) for x in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c90642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Normalize a list of positive integers\n",
    "assert min_max_scale([10, 20, 50, 30]) == [0.0, 0.25, 1.0, 0.5], \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: List containing negative numbers and zero\n",
    "assert min_max_scale([-10, 0, 10]) == [0.0, 0.5, 1.0], \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: All elements are the same, avoid division by zero\n",
    "assert min_max_scale([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0], \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Empty input list\n",
    "assert min_max_scale([]) == [], \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Data already in the range [0, 1]\n",
    "# Note: Due to floating point error, we compare with a small tolerance\n",
    "scaled_data = min_max_scale([0.1, 0.5, 0.9])\n",
    "expected_data = [0.0, 0.5, 1.0]\n",
    "assert all(abs(a - b) < 1e-9 for a, b in \\\n",
    "    zip(scaled_data, expected_data)), \"Test 5 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7803f",
   "metadata": {},
   "source": [
    "## 7. Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def calculate_accuracy(y_true: Sequence, y_pred: Sequence) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions compared to true labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (Sequence): True labels.\n",
    "        y_pred (Sequence): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy as a float between 0 and 1. \n",
    "        Returns 0 if input lists are empty or have different lengths.\n",
    "    \"\"\"\n",
    "    if not y_true or not y_pred or len(y_true) != len(y_pred):\n",
    "        return 0.0\n",
    "\n",
    "    return sum(1 for true, pred \\\n",
    "        in zip(y_true, y_pred) if true == pred) / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Labels are strings, accuracy 80%\n",
    "y_true1 = [\"cat\", \"dog\", \"cat\", \"bird\", \"dog\"]\n",
    "y_pred1 = [\"cat\", \"dog\", \"cat\", \"dog\", \"dog\"]\n",
    "assert calculate_accuracy(y_true1, y_pred1) == 0.8, \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: Labels are numbers, accuracy 100%\n",
    "y_true2 = [1, 0, 1, 1, 0]\n",
    "y_pred2 = [1, 0, 1, 1, 0]\n",
    "assert calculate_accuracy(y_true2, y_pred2) == 1.0, \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: Accuracy 0%\n",
    "y_true3 = [0, 0, 0]\n",
    "y_pred3 = [1, 1, 1]\n",
    "assert calculate_accuracy(y_true3, y_pred3) == 0.0, \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Empty lists\n",
    "assert calculate_accuracy([], []) == 0.0, \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Accuracy 50%\n",
    "y_true5 = [\"A\", \"B\"]\n",
    "y_pred5 = [\"A\", \"C\"]\n",
    "assert calculate_accuracy(y_true5, y_pred5) == 0.5, \"Test 5 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e55dac",
   "metadata": {},
   "source": [
    "## 8. Time-Series Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6526db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def add_noise_augmentation(time_series, noise_level = 1.5):\n",
    "    if not time_series:\n",
    "        return []\n",
    "    return [x + random.gauss(mu = 0, sigma = noise_level) \\\n",
    "        for x in time_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# To ensure reproducible results, set the random seed\n",
    "random.seed(0)\n",
    "\n",
    "# Test 1: Add noise to a time series\n",
    "ts1 = [10, 11, 12, 11, 10]\n",
    "augmented_ts1 = add_noise_augmentation(ts1, 0.1)\n",
    "assert len(augmented_ts1) == len(ts1), \\\n",
    "    \"Test 1 Failed: Length mismatch\"\n",
    "assert augmented_ts1 != ts1, \\\n",
    "    \"Test 1 Failed: Series should be different after adding noise\"\n",
    "\n",
    "# Test 2: noise_level = 0, series should not change\n",
    "ts2 = [100, 200, 150]\n",
    "augmented_ts2 = add_noise_augmentation(ts2, 0.0)\n",
    "assert augmented_ts2 == ts2, \\\n",
    "    \"Test 2 Failed: Series should be identical with zero noise\"\n",
    "\n",
    "# Test 3: Empty series\n",
    "assert add_noise_augmentation([], 0.5) == [], \\\n",
    "    \"Test 3 Failed: Empty list should return empty list\"\n",
    "\n",
    "# Test 4: Check that value changes\n",
    "# Since the result is random, we only check that \n",
    "# it is different from the original\n",
    "ts4 = [5]\n",
    "augmented_ts4 = add_noise_augmentation(ts4, 1.0)\n",
    "assert augmented_ts4 != ts4, \\\n",
    "    \"Test 4 Failed: Single element should change\"\n",
    "    \n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc4861b",
   "metadata": {},
   "source": [
    "## 9. Experience Replay Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7831583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "from typing import Any, List\n",
    "\n",
    "class ExperienceReplayBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-size buffer to store experience tuples for reinforcement learning.\n",
    "    Oldest experiences are discarded when the buffer is full.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            capacity (int): Maximum number of experiences to store.\n",
    "        \"\"\"\n",
    "        if capacity <= 0:\n",
    "            raise ValueError(\"Capacity must be a positive integer\")\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience: Any) -> None:\n",
    "        \"\"\"\n",
    "        Add a new experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: Typically a tuple like \n",
    "            (state, action, reward, next_state, done)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            List of experiences. If batch_size > number of experiences, \n",
    "            returns all experiences.\n",
    "        \"\"\"\n",
    "        if batch_size <= 0:\n",
    "            raise ValueError(\"batch_size must be a positive integer\")\n",
    "        actual_batch_size = min(batch_size, len(self.buffer))\n",
    "        return random.sample(self.buffer, actual_batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the current number of experiences stored.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Add and sample\n",
    "buffer = ExperienceReplayBuffer(capacity=100)\n",
    "buffer.add((\"s1\", \"a1\", 1, \"s2\"))\n",
    "buffer.add((\"s2\", \"a2\", 0, \"s3\"))\n",
    "buffer.add((\"s3\", \"a3\", -1, \"s4\"))\n",
    "assert len(buffer) == 3, \"Test 1 Failed: Incorrect buffer size\"\n",
    "sample = buffer.sample(2)\n",
    "assert len(sample) == 2, \"Test 1 Failed: Incorrect sample size\"\n",
    "\n",
    "# Test 2: Exceeding capacity\n",
    "buffer = ExperienceReplayBuffer(capacity=3)\n",
    "for i in range(5):\n",
    "    buffer.add(i)  # Add 0, 1, 2, 3, 4\n",
    "assert len(buffer) == 3, \\\n",
    "    \"Test 2 Failed: Capacity exceeded but size is wrong\"\n",
    "# Because of FIFO, remaining elements should be 2, 3, 4\n",
    "assert list(buffer.buffer) == [2, 3, 4], \\\n",
    "    \"Test 2 Failed: Old experiences not discarded correctly\"\n",
    "\n",
    "# Test 3: Sampling more than current number of elements\n",
    "buffer = ExperienceReplayBuffer(capacity=10)\n",
    "buffer.add(1)\n",
    "buffer.add(2)\n",
    "sample = buffer.sample(5)\n",
    "assert len(sample) == 2, \\\n",
    "    \"Test 3 Failed: Should return all elements if batch_size is larger\"\n",
    "\n",
    "# Test 4: Sampling from an empty buffer\n",
    "buffer = ExperienceReplayBuffer(capacity=10)\n",
    "sample = buffer.sample(5)\n",
    "assert sample == [], \\\n",
    "    \"Test 4 Failed: Sampling from empty buffer should return empty list\"\n",
    "    \n",
    "print(\"✅ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e5174",
   "metadata": {},
   "source": [
    "## 10. Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4207cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_bow_vectors(corpus, vocabulary):\n",
    "    \"\"\"\n",
    "    Convert a corpus into Bag-of-Words (BoW) vectors.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[list[str]]): List of documents, each document is a list of words.\n",
    "        vocabulary (list[str]): List of unique words that define the vector space.\n",
    "\n",
    "    Returns:\n",
    "        list[list[int]]: BoW vectors where each document is represented by \n",
    "                         word counts aligned with the vocabulary.\n",
    "    \"\"\"\n",
    "    if not corpus or not vocabulary:\n",
    "        return []\n",
    "    \n",
    "    bow_vectors = []\n",
    "    for doc in corpus:\n",
    "        counts = Counter(doc)  # count word frequencies once\n",
    "        bow_vectors.append([counts.get(word, 0) for word in vocabulary])\n",
    "    \n",
    "    return bow_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26467b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"apple\", \"banana\", \"fruit\", \"orange\"]\n",
    "\n",
    "# Test 1: Two documents with words from the vocabulary\n",
    "corpus1 = [[\"apple\", \"banana\", \"apple\"], [\"fruit\", \"orange\"]]\n",
    "expected1 = [[2, 1, 0, 0], [0, 0, 1, 1]]\n",
    "assert create_bow_vectors(corpus1, vocab) == expected1, \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: Document contains a word not in the vocabulary\n",
    "corpus2 = [[\"apple\", \"grape\", \"banana\"]]  # \"grape\" is not in vocab\n",
    "expected2 = [[1, 1, 0, 0]]\n",
    "assert create_bow_vectors(corpus2, vocab) == expected2, \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: One empty document\n",
    "corpus3 = [[]]\n",
    "expected3 = [[0, 0, 0, 0]]\n",
    "assert create_bow_vectors(corpus3, vocab) == expected3, \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Empty corpus\n",
    "assert create_bow_vectors([], vocab) == [], \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Larger vocabulary, document contains only some words\n",
    "vocab5 = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "corpus5 = [[\"a\", \"c\", \"e\", \"a\"]]\n",
    "expected5 = [[2, 0, 1, 0, 1]]\n",
    "assert create_bow_vectors(corpus5, vocab5) == expected5, \"Test 5 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
