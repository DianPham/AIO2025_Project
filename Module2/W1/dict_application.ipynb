{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c563d4f6",
   "metadata": {},
   "source": [
    "# DICTIONARY IN REAL-WORLD PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a897fe8",
   "metadata": {},
   "source": [
    "## 1. Character Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85867ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_character(word: str) -> dict:\n",
    "    \"\"\"\n",
    "    Count the frequency of each character in a given string.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input string to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are characters from the input string \n",
    "              and values are their respective counts. \n",
    "              Returns an empty dictionary if the input is empty.\n",
    "    \n",
    "    Example:\n",
    "        >>> count_character(\"hello\")\n",
    "        {'h': 1, 'e': 1, 'l': 2, 'o': 1}\n",
    "    \"\"\"\n",
    "    # Handle the case where the input string is empty or None\n",
    "    if not word:\n",
    "        return {}\n",
    "    \n",
    "    # Use a dictionary comprehension:\n",
    "    # - Convert the string into a set to get unique characters\n",
    "    # - Count occurrences of each character using str.count()\n",
    "    return {char: word.count(char) for char in set(word)}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d770473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Normal word\n",
    "assert count_character(\"hello\") == {\"h\": 1, \"e\": 1, \"l\": 2, \"o\": 1}, \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: Empty string\n",
    "assert count_character(\"\") == {}, \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: All characters the same\n",
    "assert count_character(\"aaa\") == {\"a\": 3}, \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: Word with mixed characters\n",
    "assert count_character(\"abcabc\") == {\"a\": 2, \"b\": 2, \"c\": 2}, \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: Word with spaces\n",
    "assert count_character(\"a a\") == {\"a\": 2, \" \": 1}, \"Test 5 Failed\"\n",
    "\n",
    "# Test 6: Word with uppercase and lowercase (case sensitive check)\n",
    "assert count_character(\"AaA\") == {\"A\": 2, \"a\": 1}, \"Test 6 Failed\"\n",
    "\n",
    "# Test 7: Word with digits and symbols\n",
    "assert count_character(\"a1!a\") == {\"a\": 2, \"1\": 1, \"!\": 1}, \"Test 7 Failed\"\n",
    "\n",
    "print(\"✅ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0020b8",
   "metadata": {},
   "source": [
    "## 2. Word Counting (from file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3376b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(file_path: str = 'assets/P1_data.txt') -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocess text data from a file.\n",
    "\n",
    "    Steps:\n",
    "        1. Read the file content.\n",
    "        2. Convert all text to lowercase.\n",
    "        3. Remove periods ('.') and commas (',').\n",
    "        4. Split the cleaned text into a list of words.\n",
    "\n",
    "    Args:\n",
    "        file_path (str, optional): Path to the input text file. \n",
    "                                   Defaults to 'assets/P1_data.txt'.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of processed words.\n",
    "\n",
    "    Example:\n",
    "        If the file contains: \"Hello, world. Hello!\"\n",
    "        The function returns: ['hello', 'world', 'hello']\n",
    "    \"\"\"\n",
    "    # Open and read the file content\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Normalize: lowercase, remove punctuation, and split into words\n",
    "    return text.lower().replace('.', '').replace(',', '').split()\n",
    "\n",
    "\n",
    "def count_word(words: list[str]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the frequency of each word in a list.\n",
    "\n",
    "    Args:\n",
    "        words (list[str]): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, int]: A dictionary mapping each unique word \n",
    "                        to its frequency in the list.\n",
    "                        Returns an empty dictionary if input is empty.\n",
    "\n",
    "    Example:\n",
    "        >>> count_word([\"hello\", \"world\", \"hello\"])\n",
    "        {'hello': 2, 'world': 1}\n",
    "    \"\"\"\n",
    "    if not words:\n",
    "        return {}\n",
    "    \n",
    "    # Use a dictionary comprehension:\n",
    "    # - Convert words into a set to get unique words\n",
    "    # - Count occurrences of each word using str.count()\n",
    "    return {word: words.count(word) for word in set(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade8b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with': 4, 'a': 7, 'man': 6, 'different': 1, 'usually': 1, 'everything': 1, 'your': 1, 'people': 1, 'value': 1, 'and': 1, 'have': 1, 'become': 2, 'just': 2, 'others': 1, 'small': 1, 'try': 2, 'are': 1, 'lay': 1, 'can': 3, 'conquers': 1, 'opportunity': 1, 'him': 1, 'majority': 1, 'thinking': 1, 'firm': 1, 'other': 1, 'those': 1, 'one': 4, 'foundation': 1, 'in': 4, 'you': 3, 'but': 1, 'if': 1, 'warrior': 1, 'whole': 1, 'thought': 1, 'his': 2, 'looking': 1, 'who': 3, 'will': 2, 'rather': 1, 'problems': 1, 'up': 1, 'what': 1, 'employed': 1, 'morning': 1, 'want': 2, 'thrown': 1, 'courage': 1, 'he': 1, 'it': 2, 'mistakes': 1, 'success': 3, 'we': 3, 'came': 1, 'to': 3, 'from': 1, 'busy': 1, 'them': 1, 'again': 1, 'cannot': 1, 'ready': 1, 'is': 3, 'day': 1, 'comes': 2, 'get': 2, 'successful': 2, 'way': 1, 'mightiest': 1, 'they': 1, 'be': 1, 'at': 1, 'the': 5, 'bricks': 1, 'too': 1, 'when': 2, 'profit': 1, 'life': 2, 'secret': 1, 'solve': 1, 'himself': 1, 'enough': 1, 'not': 1, 'for': 3, 'of': 4, 'kind': 1, 'positive': 1, 'change': 1, 'help': 1, 'makes': 1}\n"
     ]
    }
   ],
   "source": [
    "words = preprocess_text() # Get content from input file\n",
    "print(count_word(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc90c9f3",
   "metadata": {},
   "source": [
    "## 3. N-grams For Author Profiling Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac79623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, WordNetLemmatizer, word_tokenize, download\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class NgramProfile:\n",
    "    \"\"\"\n",
    "    A class to build n-gram profiles from input text.\n",
    "    It preprocesses text using stemming and lemmatization, \n",
    "    then extracts n-grams for text analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize NLP tools such as stemmer and lemmatizer.\"\"\"\n",
    "        self._init_nlp_tools()\n",
    "\n",
    "    def get_profile(self, text: str, n: int = 2) -> Counter:\n",
    "        \"\"\"\n",
    "        Generate an n-gram profile of the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to process.\n",
    "            n (int): Size of the n-grams (default is 2 for bigrams).\n",
    "\n",
    "        Returns:\n",
    "            Counter: A frequency dictionary of n-grams.\n",
    "        \"\"\"\n",
    "        tokens = self._process_text(text)\n",
    "        n_gram_list = self._ngrams(tokens, n)\n",
    "        profile = Counter(n_gram_list)\n",
    "        return profile\n",
    "\n",
    "    def _init_nlp_tools(self):\n",
    "        \"\"\"\n",
    "        Download required NLTK resources and initialize \n",
    "        stemmer and lemmatizer for preprocessing.\n",
    "        \"\"\"\n",
    "        download('punkt')\n",
    "        download('wordnet')\n",
    "\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def _process_text(self, text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Tokenize, lemmatize, and stem the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text string.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of processed tokens.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        processed_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Skip punctuation\n",
    "            if token in string.punctuation:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize and then stem the token\n",
    "            lemma = self.lemmatizer.lemmatize(token)\n",
    "            stem = self.stemmer.stem(lemma)\n",
    "            processed_tokens.append(stem)\n",
    "\n",
    "        return processed_tokens\n",
    "\n",
    "    def _ngrams(self, tokens: list[str], n: int = 2) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        Generate n-grams from a list of tokens.\n",
    "\n",
    "        Args:\n",
    "            tokens (list[str]): List of preprocessed tokens.\n",
    "            n (int): Size of the n-grams (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            list[tuple]: A list of n-gram tuples.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If n is not a positive integer.\n",
    "        \"\"\"\n",
    "        if n <= 0:\n",
    "            raise ValueError(\"n must be a positive integer\")\n",
    "        if len(tokens) < n:\n",
    "            return []\n",
    "\n",
    "        return [tuple(tokens[i: i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "\n",
    "class ProfileComparator:\n",
    "    \"\"\"\n",
    "    A class to compare two text profiles using cosine similarity \n",
    "    of their n-gram distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the comparator with an NgramProfile generator.\"\"\"\n",
    "        self.profile_generator = NgramProfile()\n",
    "\n",
    "    def compare_authors(self, content1: str, content2: str) -> float:\n",
    "        \"\"\"\n",
    "        Compare two texts by their n-gram profiles.\n",
    "\n",
    "        Args:\n",
    "            content1 (str): First text.\n",
    "            content2 (str): Second text.\n",
    "\n",
    "        Returns:\n",
    "            float: Cosine similarity score between the two texts.\n",
    "        \"\"\"\n",
    "        return self._get_cosine_similarity(content1, content2)\n",
    "\n",
    "    def _get_cosine_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts.\n",
    "\n",
    "        Args:\n",
    "            text1 (str): First text string.\n",
    "            text2 (str): Second text string.\n",
    "\n",
    "        Returns:\n",
    "            float: Cosine similarity score.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If one or both texts are missing.\n",
    "        \"\"\"\n",
    "        if not text1 or not text2:\n",
    "            raise ValueError(\"Both texts must be provided for comparison.\")\n",
    "\n",
    "        # Get n-gram profiles\n",
    "        profile1 = self.profile_generator.get_profile(text1)\n",
    "        profile2 = self.profile_generator.get_profile(text2)\n",
    "\n",
    "        # Union of all n-grams found in both texts\n",
    "        all_ngrams = set(profile1.keys()).union(set(profile2.keys()))\n",
    "\n",
    "        # Build frequency vectors for both profiles\n",
    "        vec1 = [profile1.get(ngram, 0) for ngram in all_ngrams]\n",
    "        vec2 = [profile2.get(ngram, 0) for ngram in all_ngrams]\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        vec1 = np.array(vec1).reshape(1, -1)\n",
    "        vec2 = np.array(vec2).reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        return cosine_similarity(vec1, vec2)[0][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
